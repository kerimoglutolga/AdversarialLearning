{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GATconditioned.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOJcm160yRVcTUlnPZu6v9G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kerimoglutolga/AdversarialLearning/blob/master/GATconditioned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KssbejInr7TO"
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F \n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPho-djehtf9"
      },
      "source": [
        "Not working conditioned GAT implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2KW-3PMsHvi",
        "outputId": "71de723d-be3c-4116-abeb-ef28016be594"
      },
      "source": [
        "!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
        "!tar -zxvf MNIST.tar.gz\n",
        "\n",
        "train_set = MNIST(\n",
        "    './', \n",
        "    download=True,\n",
        "    transform=transforms.ToTensor(), \n",
        "    train=True\n",
        ")\n",
        "\n",
        "test_set = MNIST(\n",
        "    './', \n",
        "    download=True,\n",
        "    transform=transforms.ToTensor(), \n",
        "    train=False\n",
        ")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-04 12:55:22--  http://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
            "Resolving www.di.ens.fr (www.di.ens.fr)... 129.199.99.14\n",
            "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.di.ens.fr/~lelarge/MNIST.tar.gz [following]\n",
            "--2021-10-04 12:55:23--  https://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
            "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-gzip]\n",
            "Saving to: ‘MNIST.tar.gz.10’\n",
            "\n",
            "MNIST.tar.gz.10         [           <=>      ]  33.20M  6.33MB/s    in 5.8s    \n",
            "\n",
            "2021-10-04 12:55:29 (5.76 MB/s) - ‘MNIST.tar.gz.10’ saved [34813078]\n",
            "\n",
            "MNIST/\n",
            "MNIST/raw/\n",
            "MNIST/raw/train-labels-idx1-ubyte\n",
            "MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "MNIST/raw/t10k-labels-idx1-ubyte\n",
            "MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "MNIST/raw/train-images-idx3-ubyte\n",
            "MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "MNIST/raw/t10k-images-idx3-ubyte\n",
            "MNIST/raw/train-images-idx3-ubyte.gz\n",
            "MNIST/processed/\n",
            "MNIST/processed/training.pt\n",
            "MNIST/processed/test.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRufF7gmsIXq"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=128, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=128, shuffle=False)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jdo-YARhs0el"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.label_emb = nn.Embedding(10, 10)\n",
        "\n",
        "       \n",
        "        self.add_conditions = nn.Sequential(\n",
        "        nn.Linear(28*28 + 10, 256), nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Linear(256, 784), nn.LeakyReLU(0.2, inplace=True),    \n",
        "        )\n",
        "        self.net = nn.Sequential(\n",
        "        nn.Conv2d(1, 48, kernel_size=3, stride=1, padding=1), nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Conv2d(48, 48, kernel_size=3, stride=1, padding=1), nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Conv2d(48, 48, kernel_size=3, stride=1, padding=1), nn.ReLU(),\n",
        "        nn.Conv2d(48, 48, kernel_size=3, stride=1, padding=1), nn.ReLU(),\n",
        "        nn.Conv2d(48, 48, kernel_size=3, stride=1, padding=1), nn.ReLU(),\n",
        "        nn.Conv2d(48, 48, kernel_size=3, stride=1, padding=1), nn.ReLU(),\n",
        "        nn.Conv2d(48, 1, kernel_size=1), nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "        # Concatenate label embedding and image to produce input\n",
        "        img = torch.cat((x.view(x.size(0), -1),   self.label_emb(labels)), -1)\n",
        "        img = self.add_conditions(img)\n",
        "        img = img.view(img.size(0),1, 28,28)\n",
        "        img = self.net(img)\n",
        "        return img\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyOjIK43yJWT"
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.label_emb = nn.Embedding(10, 10)\n",
        "    self.add_conditions = nn.Sequential(\n",
        "      nn.Linear(28*28 + 10, 256), nn.LeakyReLU(0.2, inplace=True),\n",
        "      nn.Linear(256, 784), nn.LeakyReLU(0.2, inplace=True),    \n",
        "    )\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Conv2d(1, 48, kernel_size=3), nn.ReLU(),\n",
        "        nn.Conv2d(48, 48, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
        "        nn.Conv2d(48, 96, kernel_size=3), nn.ReLU(),\n",
        "        nn.Conv2d(96, 96, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
        "        nn.Conv2d(96, 96, kernel_size=3, stride=1, padding=1), nn.ReLU(),\n",
        "        nn.Conv2d(96, 96, kernel_size=1, stride=1, padding=1), nn.ReLU(),\n",
        "        nn.Conv2d(96, 10, kernel_size=1), \n",
        "        nn.AvgPool2d(kernel_size=8),\n",
        "    )\n",
        "  def forward(self, x, labels):\n",
        "    img = torch.cat((x.view(x.size(0), -1),   self.label_emb(labels)), -1)\n",
        "    img = self.add_conditions(img)\n",
        "    img = img.view(img.size(0), 1, 28,28)\n",
        "    img = self.net(img)\n",
        "    logits = F.softmax(img, dim=1)\n",
        "    return logits.view(-1, 10)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESOhqAWIycPy"
      },
      "source": [
        "epochs = 20\n",
        "epsilon = 0.1\n",
        "alpha = 0.5\n",
        "cg = 0.5\n",
        "k = 1\n",
        "\n",
        "FloatTensor = torch.cuda.FloatTensor \n",
        "LongTensor = torch.cuda.LongTensor \n",
        "\n",
        "generator = Generator().cuda()\n",
        "classifier = Classifier().cuda()\n",
        "adversarial_loss = torch.nn.MSELoss()\n",
        "# Standard (non-adversarial) training loop\n",
        "def train(device, train_loader):\n",
        "  generator_optimizer = torch.optim.Adam(generator.parameters(), lr=1e-6)\n",
        "  classifier_optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3)\n",
        "  loss = nn.CrossEntropyLoss()\n",
        "  for epoch in range(epochs):\n",
        "    for i, (x,y) in enumerate(train_loader):\n",
        "      x, y = x.to(device), y.to(device)\n",
        "\n",
        "      valid = Variable(FloatTensor(y.size(0), 1).fill_(1.0), requires_grad=False)\n",
        "      fake = Variable(FloatTensor(y.size(0), 1).fill_(0.0), requires_grad=False)\n",
        "\n",
        "\n",
        "      gen_labels = Variable(LongTensor(np.random.randint(0, 10, y.size(0))))\n",
        "      \n",
        "    \n",
        "      generator_optimizer.zero_grad()\n",
        "      perturbation = generator(x, gen_labels)\n",
        "      probs = F.softmax(classifier(x + perturbation, gen_labels), dim=1)\n",
        "      batch_loss = probs + cg * torch.norm(perturbation, p=2)\n",
        "      validity_loss = loss(probs, gen_labels)\n",
        "      gen_loss = 0.5*batch_loss.mean() + 0.5*validity_loss\n",
        "      gen_loss.backward()\n",
        "      generator_optimizer.step()\n",
        "\n",
        "      \"\"\"try:\n",
        "        x, y = next(train_loader_iter)\n",
        "      except StopIteration:\n",
        "        train_loader_iter = iter(train_loader)\n",
        "        x, y = next(train_loader_iter)\n",
        "      x,y = x.to(device), y.to(device)\"\"\"\n",
        "      classifier_optimizer.zero_grad()\n",
        "      perturbation = generator(x, gen_labels)\n",
        "      classifier_loss = alpha * loss(classifier(x, gen_labels), y) + (1-alpha) * loss(classifier(x+perturbation, gen_labels), y) + loss(classifier(x+perturbation, gen_labels), gen_labels)\n",
        "      classifier_loss.backward()\n",
        "      classifier_optimizer.step()      \n",
        "      if (i%100 == 0):\n",
        "        print('Epoch [{}/{}], Step [{}/{}], Generator_loss: {}, Classifier_loss: {}'.format(epoch+1, epochs, i+1, len(train_loader), gen_loss, classifier_loss))\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NW2-u3DBNW4L",
        "outputId": "94a49590-014d-4fb7-f783-37f0ea7306a7"
      },
      "source": [
        "train(device, train_loader) "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Step [1/469], Generator_loss: 2.565598487854004, Classifier_loss: 4.60506534576416\n",
            "Epoch [1/20], Step [101/469], Generator_loss: 2.3501930236816406, Classifier_loss: 4.37032413482666\n",
            "Epoch [1/20], Step [201/469], Generator_loss: 2.1378402709960938, Classifier_loss: 4.198477745056152\n",
            "Epoch [1/20], Step [301/469], Generator_loss: 1.9095795154571533, Classifier_loss: 4.040802001953125\n",
            "Epoch [1/20], Step [401/469], Generator_loss: 1.6887869834899902, Classifier_loss: 3.9816696643829346\n",
            "Epoch [2/20], Step [1/469], Generator_loss: 1.549318552017212, Classifier_loss: 3.946845531463623\n",
            "Epoch [2/20], Step [101/469], Generator_loss: 1.4010130167007446, Classifier_loss: 3.9434688091278076\n",
            "Epoch [2/20], Step [201/469], Generator_loss: 1.3652894496917725, Classifier_loss: 3.9540655612945557\n",
            "Epoch [2/20], Step [301/469], Generator_loss: 1.3508189916610718, Classifier_loss: 3.954996109008789\n",
            "Epoch [2/20], Step [401/469], Generator_loss: 1.3372236490249634, Classifier_loss: 3.9300198554992676\n",
            "Epoch [3/20], Step [1/469], Generator_loss: 1.3338017463684082, Classifier_loss: 4.016805171966553\n",
            "Epoch [3/20], Step [101/469], Generator_loss: 1.3223658800125122, Classifier_loss: 3.933692216873169\n",
            "Epoch [3/20], Step [201/469], Generator_loss: 1.3092024326324463, Classifier_loss: 3.9067699909210205\n",
            "Epoch [3/20], Step [301/469], Generator_loss: 1.301849603652954, Classifier_loss: 3.8897485733032227\n",
            "Epoch [3/20], Step [401/469], Generator_loss: 1.2885509729385376, Classifier_loss: 3.9139304161071777\n",
            "Epoch [4/20], Step [1/469], Generator_loss: 1.2786734104156494, Classifier_loss: 3.9401791095733643\n",
            "Epoch [4/20], Step [101/469], Generator_loss: 1.2625796794891357, Classifier_loss: 3.929295778274536\n",
            "Epoch [4/20], Step [201/469], Generator_loss: 1.2512750625610352, Classifier_loss: 3.8666441440582275\n",
            "Epoch [4/20], Step [301/469], Generator_loss: 1.239895224571228, Classifier_loss: 3.9772019386291504\n",
            "Epoch [4/20], Step [401/469], Generator_loss: 1.2293827533721924, Classifier_loss: 3.9176394939422607\n",
            "Epoch [5/20], Step [1/469], Generator_loss: 1.2132929563522339, Classifier_loss: 3.8896217346191406\n",
            "Epoch [5/20], Step [101/469], Generator_loss: 1.2009670734405518, Classifier_loss: 3.925431728363037\n",
            "Epoch [5/20], Step [201/469], Generator_loss: 1.1880931854248047, Classifier_loss: 3.8431310653686523\n",
            "Epoch [5/20], Step [301/469], Generator_loss: 1.1745007038116455, Classifier_loss: 3.812296152114868\n",
            "Epoch [5/20], Step [401/469], Generator_loss: 1.1686092615127563, Classifier_loss: 3.8298885822296143\n",
            "Epoch [6/20], Step [1/469], Generator_loss: 1.1648461818695068, Classifier_loss: 3.7975027561187744\n",
            "Epoch [6/20], Step [101/469], Generator_loss: 1.1633315086364746, Classifier_loss: 3.812700033187866\n",
            "Epoch [6/20], Step [201/469], Generator_loss: 1.1612272262573242, Classifier_loss: 3.8282556533813477\n",
            "Epoch [6/20], Step [301/469], Generator_loss: 1.1600115299224854, Classifier_loss: 3.7969913482666016\n",
            "Epoch [6/20], Step [401/469], Generator_loss: 1.158923625946045, Classifier_loss: 3.8129262924194336\n",
            "Epoch [7/20], Step [1/469], Generator_loss: 1.158243179321289, Classifier_loss: 3.8207340240478516\n",
            "Epoch [7/20], Step [101/469], Generator_loss: 1.1574162244796753, Classifier_loss: 3.8441762924194336\n",
            "Epoch [7/20], Step [201/469], Generator_loss: 1.1566238403320312, Classifier_loss: 3.844170093536377\n",
            "Epoch [7/20], Step [301/469], Generator_loss: 1.1564775705337524, Classifier_loss: 3.851989269256592\n",
            "Epoch [7/20], Step [401/469], Generator_loss: 1.1550447940826416, Classifier_loss: 3.875426769256592\n",
            "Epoch [8/20], Step [1/469], Generator_loss: 1.1545495986938477, Classifier_loss: 3.812926769256592\n",
            "Epoch [8/20], Step [101/469], Generator_loss: 1.1538059711456299, Classifier_loss: 3.781676769256592\n",
            "Epoch [8/20], Step [201/469], Generator_loss: 1.1530036926269531, Classifier_loss: 3.844176769256592\n",
            "Epoch [8/20], Step [301/469], Generator_loss: 1.1521879434585571, Classifier_loss: 3.820739269256592\n",
            "Epoch [8/20], Step [401/469], Generator_loss: 1.1513735055923462, Classifier_loss: 3.875426769256592\n",
            "Epoch [9/20], Step [1/469], Generator_loss: 1.15090811252594, Classifier_loss: 3.805114269256592\n",
            "Epoch [9/20], Step [101/469], Generator_loss: 1.1500881910324097, Classifier_loss: 3.812926769256592\n",
            "Epoch [9/20], Step [201/469], Generator_loss: 1.1494280099868774, Classifier_loss: 3.836364269256592\n",
            "Epoch [9/20], Step [301/469], Generator_loss: 1.148516297340393, Classifier_loss: 3.828551769256592\n",
            "Epoch [9/20], Step [401/469], Generator_loss: 1.147840976715088, Classifier_loss: 3.805114269256592\n",
            "Epoch [10/20], Step [1/469], Generator_loss: 1.1478724479675293, Classifier_loss: 3.7571754455566406\n",
            "Epoch [10/20], Step [101/469], Generator_loss: 1.204430103302002, Classifier_loss: 4.569453239440918\n",
            "Epoch [10/20], Step [201/469], Generator_loss: 1.2128435373306274, Classifier_loss: 4.805114269256592\n",
            "Epoch [10/20], Step [301/469], Generator_loss: 1.2069469690322876, Classifier_loss: 4.656676769256592\n",
            "Epoch [10/20], Step [401/469], Generator_loss: 1.2093313932418823, Classifier_loss: 4.680112361907959\n",
            "Epoch [11/20], Step [1/469], Generator_loss: 1.210623860359192, Classifier_loss: 4.758239269256592\n",
            "Epoch [11/20], Step [101/469], Generator_loss: 1.2076530456542969, Classifier_loss: 4.648864269256592\n",
            "Epoch [11/20], Step [201/469], Generator_loss: 1.2094273567199707, Classifier_loss: 4.711364269256592\n",
            "Epoch [11/20], Step [301/469], Generator_loss: 1.2089438438415527, Classifier_loss: 4.719176769256592\n",
            "Epoch [11/20], Step [401/469], Generator_loss: 1.2071959972381592, Classifier_loss: 4.656674861907959\n",
            "Epoch [12/20], Step [1/469], Generator_loss: 1.2064149379730225, Classifier_loss: 4.711364269256592\n",
            "Epoch [12/20], Step [101/469], Generator_loss: 1.209486484527588, Classifier_loss: 4.742614269256592\n",
            "Epoch [12/20], Step [201/469], Generator_loss: 1.2097028493881226, Classifier_loss: 4.734801769256592\n",
            "Epoch [12/20], Step [301/469], Generator_loss: 1.207712173461914, Classifier_loss: 4.695739269256592\n",
            "Epoch [12/20], Step [401/469], Generator_loss: 1.2051653861999512, Classifier_loss: 4.656676769256592\n",
            "Epoch [13/20], Step [1/469], Generator_loss: 1.2039679288864136, Classifier_loss: 4.609801769256592\n",
            "Epoch [13/20], Step [101/469], Generator_loss: 1.2071151733398438, Classifier_loss: 4.742614269256592\n",
            "Epoch [13/20], Step [201/469], Generator_loss: 1.2039921283721924, Classifier_loss: 4.687926292419434\n",
            "Epoch [13/20], Step [301/469], Generator_loss: 1.2061127424240112, Classifier_loss: 4.648864269256592\n",
            "Epoch [13/20], Step [401/469], Generator_loss: 1.207703709602356, Classifier_loss: 4.719176769256592\n",
            "Epoch [14/20], Step [1/469], Generator_loss: 1.2042618989944458, Classifier_loss: 4.625426769256592\n",
            "Epoch [14/20], Step [101/469], Generator_loss: 1.2068891525268555, Classifier_loss: 4.766051769256592\n",
            "Epoch [14/20], Step [201/469], Generator_loss: 1.2004002332687378, Classifier_loss: 4.641051769256592\n",
            "Epoch [14/20], Step [301/469], Generator_loss: 1.2025773525238037, Classifier_loss: 4.6555023193359375\n",
            "Epoch [14/20], Step [401/469], Generator_loss: 1.2024202346801758, Classifier_loss: 4.687926769256592\n",
            "Epoch [15/20], Step [1/469], Generator_loss: 1.2042123079299927, Classifier_loss: 4.703551769256592\n",
            "Epoch [15/20], Step [101/469], Generator_loss: 1.204067349433899, Classifier_loss: 4.656676769256592\n",
            "Epoch [15/20], Step [201/469], Generator_loss: 1.205085039138794, Classifier_loss: 4.726989269256592\n",
            "Epoch [15/20], Step [301/469], Generator_loss: 1.2049661874771118, Classifier_loss: 4.726989269256592\n",
            "Epoch [15/20], Step [401/469], Generator_loss: 1.2020007371902466, Classifier_loss: 4.632770538330078\n",
            "Epoch [16/20], Step [1/469], Generator_loss: 1.204805850982666, Classifier_loss: 4.766051769256592\n",
            "Epoch [16/20], Step [101/469], Generator_loss: 1.2030771970748901, Classifier_loss: 4.687926769256592\n",
            "Epoch [16/20], Step [201/469], Generator_loss: 1.2052937746047974, Classifier_loss: 4.726989269256592\n",
            "Epoch [16/20], Step [301/469], Generator_loss: 1.2028266191482544, Classifier_loss: 4.711364269256592\n",
            "Epoch [16/20], Step [401/469], Generator_loss: 1.2045931816101074, Classifier_loss: 4.726989269256592\n",
            "Epoch [17/20], Step [1/469], Generator_loss: 1.201583743095398, Classifier_loss: 4.672301769256592\n",
            "Epoch [17/20], Step [101/469], Generator_loss: 1.2038168907165527, Classifier_loss: 4.742614269256592\n",
            "Epoch [17/20], Step [201/469], Generator_loss: 1.2037843465805054, Classifier_loss: 4.734801769256592\n",
            "Epoch [17/20], Step [301/469], Generator_loss: 1.203108310699463, Classifier_loss: 4.758239269256592\n",
            "Epoch [17/20], Step [401/469], Generator_loss: 1.1985094547271729, Classifier_loss: 4.656676769256592\n",
            "Epoch [18/20], Step [1/469], Generator_loss: 1.2052778005599976, Classifier_loss: 4.758239269256592\n",
            "Epoch [18/20], Step [101/469], Generator_loss: 1.2029454708099365, Classifier_loss: 4.711364269256592\n",
            "Epoch [18/20], Step [201/469], Generator_loss: 1.2035168409347534, Classifier_loss: 4.726989269256592\n",
            "Epoch [18/20], Step [301/469], Generator_loss: 1.202903151512146, Classifier_loss: 4.711364269256592\n",
            "Epoch [18/20], Step [401/469], Generator_loss: 1.2033721208572388, Classifier_loss: 4.719176769256592\n",
            "Epoch [19/20], Step [1/469], Generator_loss: 1.2022794485092163, Classifier_loss: 4.687926769256592\n",
            "Epoch [19/20], Step [101/469], Generator_loss: 1.2044453620910645, Classifier_loss: 4.789489269256592\n",
            "Epoch [19/20], Step [201/469], Generator_loss: 1.2003824710845947, Classifier_loss: 4.719176769256592\n",
            "Epoch [19/20], Step [301/469], Generator_loss: 1.1991838216781616, Classifier_loss: 4.711364269256592\n",
            "Epoch [19/20], Step [401/469], Generator_loss: 1.204326868057251, Classifier_loss: 4.742614269256592\n",
            "Epoch [20/20], Step [1/469], Generator_loss: 1.2013942003250122, Classifier_loss: 4.703551769256592\n",
            "Epoch [20/20], Step [101/469], Generator_loss: 1.2025213241577148, Classifier_loss: 4.687926769256592\n",
            "Epoch [20/20], Step [201/469], Generator_loss: 1.2053422927856445, Classifier_loss: 4.750426769256592\n",
            "Epoch [20/20], Step [301/469], Generator_loss: 1.197281002998352, Classifier_loss: 4.648864269256592\n",
            "Epoch [20/20], Step [401/469], Generator_loss: 1.2006556987762451, Classifier_loss: 4.687926769256592\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxxOdWIyQg__",
        "outputId": "fa2a3547-cf99-4df5-a48e-eae5c7f25fe1"
      },
      "source": [
        "total = 0\n",
        "adv_correct = 0\n",
        "for x, y in test_loader:\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  gen_labels = Variable(LongTensor(np.random.randint(0, 10, y.size(0))))\n",
        "  perturbation = generator(x, gen_labels)\n",
        "  perturbed_image = x + perturbation\n",
        "\n",
        "  adv_out = classifier(perturbed_image, gen_labels)\n",
        "  _, adv_pred = torch.max(adv_out.data, 1)\n",
        "  total += y.size(0)\n",
        "  adv_correct += (adv_pred == y).sum().item()\n",
        "\n",
        "print(\"Adversarially trained network's accuracy:\", adv_correct/total)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adversarially trained network's accuracy: 0.0892\n"
          ]
        }
      ]
    }
  ]
}